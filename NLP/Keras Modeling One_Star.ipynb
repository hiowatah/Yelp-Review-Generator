{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing necessary libraries to run models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pickle import dump\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from pickle import load\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Need to load my 5 txt files to make the 5 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loader(textfile):\n",
    "    # Open the file as read-only\n",
    "    file = open(textfile, 'r')\n",
    "    # Actually read the file\n",
    "    text = file.read()\n",
    "    # Now that I have the data in my text variable, need to close out of the file\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 5 instances of the 5 ratings\n",
    "\n",
    "one = loader('One_Star.txt')\n",
    "# two = loader('Two_Star.txt')\n",
    "# three = loader('Three_Star.txt')\n",
    "# four = loader('Four_Star.txt')\n",
    "# five = loader('Five_Star.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to split up each of these data sets into training sequences by the different lines. \n",
    "# End goal is to have user input 5 words and the text generator will work its magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_lines = one.split('\\n')\n",
    "# two_lines = two.split('\\n')\n",
    "# three_lines = three.split('\\n')\n",
    "# four_lines = four.split('\\n')\n",
    "# five_lines = five.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['atrocious the live music was loud',\n",
       " 'the live music was loud and',\n",
       " 'live music was loud and mediocre',\n",
       " 'music was loud and mediocre the',\n",
       " 'was loud and mediocre the decor']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each line has been broken out with 5 sequence of words\n",
    "one_lines[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Need to convert the words in my data to integers in order to be used by the model. \n",
    "\n",
    "Will need to train the keras tokenizer on entire dataset to assign a unique ID to each unique word in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "def word_to_int(lines):\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    sequences = tokenizer.texts_to_sequences(one_lines)\n",
    "    # Need the total size of the vocabulary for our embedding layer in model\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    return sequences, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the function for all 5 data sets\n",
    "one_sequence, one_vocab_size = word_to_int(one_lines)\n",
    "# two_sequence, two_vocab_size = word_to_int(two_lines)\n",
    "# three_sequence, three_vocab_size = word_to_int(three_lines)\n",
    "# four_sequence, four_vocab_size = word_to_int(four_lines)\n",
    "# five_sequence, five_vocab_size = word_to_int(five_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2374, 1, 551, 549, 7, 809],\n",
       " [1, 551, 549, 7, 809, 3],\n",
       " [551, 549, 7, 809, 3, 1044],\n",
       " [549, 7, 809, 3, 1044, 1],\n",
       " [7, 809, 3, 1044, 1, 1081]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each line has been converted to unique ints to represent each word\n",
    "one_sequence[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_one = one_sequence[0:200000]\n",
    "\n",
    "part_two = one_sequence[200001:400000]\n",
    "\n",
    "part_three = one_sequence[400001:600000]\n",
    "\n",
    "part_four = one_sequence[600001:800000]\n",
    "\n",
    "part_five = one_sequence[800001:1000000]\n",
    "\n",
    "part_six = one_sequence[1000001:1200000]\n",
    "\n",
    "part_seven = one_sequence[1200001:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Need to convert my sequences into inputs and outputs to structure my model to predict words based on previous set of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Star Review Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_sequence = np.array(part_one)\n",
    "X, y = one_sequence[:,:-1], one_sequence[:,-1]\n",
    "y = to_categorical(y, num_classes=one_vocab_size)\n",
    "seq_length = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 5, 50)             1883750   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 5, 25)             7600      \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 25)                5100      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 25)                650       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 37675)             979550    \n",
      "=================================================================\n",
      "Total params: 2,876,650\n",
      "Trainable params: 2,876,650\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# defining my model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(one_vocab_size, 50, input_length=seq_length))\n",
    "model.add(LSTM(25, return_sequences=True))\n",
    "model.add(LSTM(25))\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dense(one_vocab_size, activation = 'softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "200000/200000 [==============================] - 244s 1ms/step - loss: 6.9837 - acc: 0.0470\n",
      "Epoch 2/50\n",
      "200000/200000 [==============================] - 261s 1ms/step - loss: 6.5298 - acc: 0.0526\n",
      "Epoch 3/50\n",
      "200000/200000 [==============================] - 269s 1ms/step - loss: 6.3249 - acc: 0.0655\n",
      "Epoch 4/50\n",
      "200000/200000 [==============================] - 262s 1ms/step - loss: 6.1356 - acc: 0.0808\n",
      "Epoch 5/50\n",
      "200000/200000 [==============================] - 254s 1ms/step - loss: 5.9557 - acc: 0.0958\n",
      "Epoch 6/50\n",
      "200000/200000 [==============================] - 255s 1ms/step - loss: 5.8238 - acc: 0.1027\n",
      "Epoch 7/50\n",
      "200000/200000 [==============================] - 237s 1ms/step - loss: 5.7190 - acc: 0.1087\n",
      "Epoch 8/50\n",
      "200000/200000 [==============================] - 235s 1ms/step - loss: 5.6188 - acc: 0.1155\n",
      "Epoch 9/50\n",
      "200000/200000 [==============================] - 234s 1ms/step - loss: 5.5250 - acc: 0.1219\n",
      "Epoch 10/50\n",
      "200000/200000 [==============================] - 235s 1ms/step - loss: 5.4408 - acc: 0.1276\n",
      "Epoch 11/50\n",
      "200000/200000 [==============================] - 236s 1ms/step - loss: 5.3650 - acc: 0.1322\n",
      "Epoch 12/50\n",
      "200000/200000 [==============================] - 236s 1ms/step - loss: 5.2927 - acc: 0.1373\n",
      "Epoch 13/50\n",
      "200000/200000 [==============================] - 236s 1ms/step - loss: 5.2271 - acc: 0.1420\n",
      "Epoch 14/50\n",
      "200000/200000 [==============================] - 236s 1ms/step - loss: 5.1640 - acc: 0.1462\n",
      "Epoch 15/50\n",
      "200000/200000 [==============================] - 236s 1ms/step - loss: 5.1052 - acc: 0.1509\n",
      "Epoch 16/50\n",
      "200000/200000 [==============================] - 236s 1ms/step - loss: 5.0494 - acc: 0.1548\n",
      "Epoch 17/50\n",
      "200000/200000 [==============================] - 235s 1ms/step - loss: 4.9956 - acc: 0.1585\n",
      "Epoch 18/50\n",
      "200000/200000 [==============================] - 236s 1ms/step - loss: 4.9440 - acc: 0.1615\n",
      "Epoch 19/50\n",
      "200000/200000 [==============================] - 237s 1ms/step - loss: 4.8936 - acc: 0.1649\n",
      "Epoch 20/50\n",
      "200000/200000 [==============================] - 236s 1ms/step - loss: 4.8471 - acc: 0.1687\n",
      "Epoch 21/50\n",
      "200000/200000 [==============================] - 237s 1ms/step - loss: 4.8010 - acc: 0.1715\n",
      "Epoch 22/50\n",
      "200000/200000 [==============================] - 237s 1ms/step - loss: 4.7570 - acc: 0.1746\n",
      "Epoch 23/50\n",
      "200000/200000 [==============================] - 238s 1ms/step - loss: 4.7130 - acc: 0.1774\n",
      "Epoch 24/50\n",
      "200000/200000 [==============================] - 243s 1ms/step - loss: 4.6727 - acc: 0.1809\n",
      "Epoch 25/50\n",
      "200000/200000 [==============================] - 245s 1ms/step - loss: 4.6321 - acc: 0.1828\n",
      "Epoch 26/50\n",
      "200000/200000 [==============================] - 243s 1ms/step - loss: 4.5922 - acc: 0.1872\n",
      "Epoch 27/50\n",
      "200000/200000 [==============================] - 242s 1ms/step - loss: 4.5555 - acc: 0.1896\n",
      "Epoch 28/50\n",
      "200000/200000 [==============================] - 242s 1ms/step - loss: 4.5185 - acc: 0.1930\n",
      "Epoch 29/50\n",
      "200000/200000 [==============================] - 244s 1ms/step - loss: 4.4844 - acc: 0.1969\n",
      "Epoch 30/50\n",
      "200000/200000 [==============================] - 244s 1ms/step - loss: 4.4495 - acc: 0.2005\n",
      "Epoch 31/50\n",
      "200000/200000 [==============================] - 243s 1ms/step - loss: 4.4174 - acc: 0.2041\n",
      "Epoch 32/50\n",
      "200000/200000 [==============================] - 247s 1ms/step - loss: 4.3861 - acc: 0.2066\n",
      "Epoch 33/50\n",
      "200000/200000 [==============================] - 246s 1ms/step - loss: 4.3541 - acc: 0.2101\n",
      "Epoch 34/50\n",
      "200000/200000 [==============================] - 244s 1ms/step - loss: 4.3245 - acc: 0.2132\n",
      "Epoch 35/50\n",
      "200000/200000 [==============================] - 245s 1ms/step - loss: 4.2964 - acc: 0.2166\n",
      "Epoch 36/50\n",
      "200000/200000 [==============================] - 243s 1ms/step - loss: 4.2690 - acc: 0.2198\n",
      "Epoch 37/50\n",
      "200000/200000 [==============================] - 245s 1ms/step - loss: 4.2422 - acc: 0.2226\n",
      "Epoch 38/50\n",
      "200000/200000 [==============================] - 247s 1ms/step - loss: 4.2159 - acc: 0.2263\n",
      "Epoch 39/50\n",
      "200000/200000 [==============================] - 247s 1ms/step - loss: 4.1898 - acc: 0.2288\n",
      "Epoch 40/50\n",
      "200000/200000 [==============================] - 246s 1ms/step - loss: 4.1646 - acc: 0.2326\n",
      "Epoch 41/50\n",
      "200000/200000 [==============================] - 246s 1ms/step - loss: 4.1417 - acc: 0.2356\n",
      "Epoch 42/50\n",
      "200000/200000 [==============================] - 246s 1ms/step - loss: 4.1171 - acc: 0.2387\n",
      "Epoch 43/50\n",
      "200000/200000 [==============================] - 246s 1ms/step - loss: 4.0958 - acc: 0.2404\n",
      "Epoch 44/50\n",
      "200000/200000 [==============================] - 246s 1ms/step - loss: 4.0760 - acc: 0.2444\n",
      "Epoch 45/50\n",
      "200000/200000 [==============================] - 246s 1ms/step - loss: 4.0525 - acc: 0.2466\n",
      "Epoch 46/50\n",
      "200000/200000 [==============================] - 247s 1ms/step - loss: 4.0343 - acc: 0.2492\n",
      "Epoch 47/50\n",
      "200000/200000 [==============================] - 248s 1ms/step - loss: 4.0135 - acc: 0.2517\n",
      "Epoch 48/50\n",
      "200000/200000 [==============================] - 248s 1ms/step - loss: 3.9919 - acc: 0.2554\n",
      "Epoch 49/50\n",
      "200000/200000 [==============================] - 247s 1ms/step - loss: 3.9778 - acc: 0.2569\n",
      "Epoch 50/50\n",
      "200000/200000 [==============================] - 248s 1ms/step - loss: 3.9594 - acc: 0.2596\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12474070f0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X,y,batch_size=128, epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model = load_model('model.h5')\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_int(lines):\n",
    "#     tokenizer.fit_on_texts(lines)\n",
    "    sequences = tokenizer.texts_to_sequences(one_lines)\n",
    "    # Need the total size of the vocabulary for our embedding layer in model\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    return sequences, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_sequence = np.array(part_two)\n",
    "X, y = one_sequence[:,:-1], one_sequence[:,-1]\n",
    "y = to_categorical(y, num_classes=one_vocab_size)\n",
    "seq_length = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      " 38272/199999 [====>.........................] - ETA: 3:08 - loss: 7.0376 - acc: 0.0973"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-284c213adf5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X,y,batch_size=128, epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
